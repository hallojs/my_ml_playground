{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_dpsgd","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPJ6sH58rXL9yEO2V4YPuBF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VV_ZlSFz_Nfb","colab_type":"text"},"source":["# MNIST DP-SGD Keras"]},{"cell_type":"markdown","metadata":{"id":"4Jvqj7hz_b2P","colab_type":"text"},"source":["# All needed Imports and Functions"]},{"cell_type":"code","metadata":{"id":"ecJsF0D9CmkH","colab_type":"code","outputId":"39a74b54-88cc-4fae-bc85-41f8ee8bec2e","executionInfo":{"status":"ok","timestamp":1586599063472,"user_tz":-120,"elapsed":24089,"user":{"displayName":"jonas sander","photoUrl":"","userId":"00193758946461779324"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["\"\"\"Evaluate the DP-SGD optimizer using TF 1.x.\n","\n","Code of the Notebook is based on\n","https://github.com/tensorflow/privacy/blob/master/tutorials/\n","mnist_dpsgd_tutorial_keras.py. For a quick walktrough see\n","https://github.com/tensorflow/privacy/tree/master/tutorials/walkthrough.\n","\n","Attributes\n","----------\n","GradientDescentOptimizer : tf.train.GradientDescentOptimizer\n","    Non-DP optimizer for the training.\n","\n","License of the code underlying this notebook\n","--------------------------------------------\n","Copyright 2019, The TensorFlow Authors.\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","     http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License.\n","\"\"\"\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import numpy as np\n","import pandas as pd\n","from timeit import default_timer as timer\n","\n","# set tensorlfow version in google colab\n","try:\n","  %tensorflow_version 1.x\n","except Exception:\n","  pass\n","import tensorflow.compat.v1 as tf\n","\n","# used to measure the privacy gurantee\n","from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n","from tensorflow_privacy.privacy.analysis.rdp_accountant import \\\n","    get_privacy_spent\n","\n","# optimizer used for the privacy-preserving training\n","from tensorflow_privacy.privacy.optimizers.dp_optimizer import \\\n","    DPGradientDescentGaussianOptimizer\n","\n","# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","GradientDescentOptimizer = tf.train.GradientDescentOptimizer\n","\n","\n","def compute_epsilon(noise_multiplier, minibatches, epochs):\n","  \"\"\"Computes epsilon value for given hyperparameters.\n","\n","      Epsilon describes the strength of our privacy guarantee. In the case of\n","      DP-ML, it gives a bound on how much the probability of a particular model\n","      output can vary by including (or removing) a single training example. We\n","      usually want it to be a small constant. However, this is only an upper\n","      bound, and a large value of epsilon could still mean good practical\n","      privacy. Interpreting this value could be quiet difficult.\n","\n","  Returns\n","  -------\n","  float\n","      Epsion-value for the expanded privacy budget.\n","\n","  Parameters\n","  ----------\n","  noise_multiplier : float\n","      Description\n","  minibatches : int\n","      Number of samples used in each training step.\n","  epochs : int\n","      Number of training iterations.\n","  \"\"\"\n","  # Together with the noise multiplier are these the parameters which are\n","  # relevant to measuring the potential privacy loss induced by the training.\n","  #\n","  # *sampling_probability: The probability of an individual training point\n","  # being included in a minibatch.\n","  # *steps: Number of steps the optimizer takes over the training data.\n","  steps = epochs * 60000 // minibatches\n","  sampling_probability = minibatches / 60000\n","\n","  if noise_multiplier == 0.0:\n","    return float('inf')\n","\n","  # The exact meaning of the orders is not quite clear to me yet. For a rough\n","  # description and a rule of thumb see the walkthrough.\n","  orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n","\n","  # Rule of thump: Delta is set to 1e-5 because MNIST has 60000 training\n","  # points (see the walktrough). Delta bounds the probability that our privacy\n","  # guarantee do not hold.\n","  rdp = compute_rdp(q=sampling_probability,\n","                    noise_multiplier=noise_multiplier,\n","                    steps=steps,\n","                    orders=orders)\n","\n","  return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n","\n","\n","def load_mnist():\n","  \"\"\"Loads MNIST and preprocesses to combine training and validation data.\n","\n","  Returns\n","  -------\n","  tuple\n","      (history of the training, runtime measurement, epsilon-value)\n","  \"\"\"\n","  train, test = tf.keras.datasets.mnist.load_data()\n","  train_data, train_labels = train\n","  test_data, test_labels = test\n","\n","  train_data = np.array(train_data, dtype=np.float32) / 255\n","  test_data = np.array(test_data, dtype=np.float32) / 255\n","\n","  train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n","  test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n","\n","  train_labels = np.array(train_labels, dtype=np.int32)\n","  test_labels = np.array(test_labels, dtype=np.int32)\n","\n","  train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n","  test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n","\n","  assert train_data.min() == 0.\n","  assert train_data.max() == 1.\n","  assert test_data.min() == 0.\n","  assert test_data.max() == 1.\n","\n","  return train_data, train_labels, test_data, test_labels\n","\n","\n","def main(dpsgd, learning_rate, noise_multiplier, l2_norm_clip, minibatches,\n","         epochs, microbatches, verbose=1):\n","  \"\"\"Define, train and compute the used privacy budget of the keras model.\n","\n","  Raises\n","  ------\n","  ValueError\n","      The number of microbatches must divide the batch size.\n","\n","  Parameters\n","  ----------\n","  dpsgd : bool\n","      If True, train with DP-SGD. If False, train with vanilla SGD.\n","  learning_rate : float\n","      Learning rate for training.\n","  noise_multiplier : float\n","      Ratio of the standard deviation to the clipping norm. Typically more\n","      noise results in stronger privacy and often at the expense of utility.\n","  l2_norm_clip : float\n","      Attribute gives the maximum Euclidean norm of each individual gradient\n","      that is computed on an individual training example from a minibatch. This\n","      parameter is used to bound the optimizer's sensitivity to individual\n","      training points.\n","  minibatches : int\n","      Number of samples used in each training step.\n","  epochs : int\n","      Number of epochs used for the training.\n","  microbatches : int\n","      Number of microbatches (must be evently divide batch size). In practice\n","      clipping gradients for each exampe indivdudally can strongly degrade the\n","      performance because instead of parallelizing at the granularity of\n","      minibatches the computations must be performed for each example. Rather\n","      than clipping gradients per example we clip them on the basis of\n","      microbatches. In this way is the number of microbatches a trade-off\n","      parameter between privacy and utility (small number -> higher privacy,\n","      number closer to size of minibatches -> higher utility).\n","  verbose : int, optional\n","      Verbose parameter of the TF fit function.\n","\n","  Returns\n","  -------\n","  TYPE\n","      Description\n","  \"\"\"\n","  if dpsgd and minibatches % microbatches != 0:\n","    raise ValueError('Number of microbatches should divide evenly minibatches')\n","\n","  # Load training and test data.\n","  train_data, train_labels, test_data, test_labels = load_mnist()\n","\n","  # Define a sequential Keras model\n","  model = tf.keras.Sequential([\n","      tf.keras.layers.Conv2D(16, 8,\n","                             strides=2,\n","                             padding='same',\n","                             activation='relu',\n","                             input_shape=(28, 28, 1)),\n","      tf.keras.layers.MaxPool2D(2, 1),\n","      tf.keras.layers.Conv2D(32, 4,\n","                             strides=2,\n","                             padding='valid',\n","                             activation='relu'),\n","      tf.keras.layers.MaxPool2D(2, 1),\n","      tf.keras.layers.Flatten(),\n","      tf.keras.layers.Dense(32, activation='relu'),\n","      tf.keras.layers.Dense(10)\n","  ])\n","\n","  if dpsgd:\n","    optimizer = DPGradientDescentGaussianOptimizer(\n","        l2_norm_clip=l2_norm_clip,\n","        noise_multiplier=noise_multiplier,\n","        num_microbatches=microbatches,\n","        learning_rate=learning_rate)\n","    # Compute vector of per-example loss rather than its mean over a minibatch.\n","    # The optimizers needs the loss per example in order to compute the\n","    # gradients per example (rather than per minibatch) and clip/noise the\n","    # gradient of each example individually.\n","    loss = tf.keras.losses.CategoricalCrossentropy(\n","        from_logits=True, reduction=tf.losses.Reduction.NONE)\n","  else:\n","    optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n","    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","\n","  # Compile model with Keras\n","  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n","\n","  # Train model with Keras and measure training time\n","  start = timer()\n","  history = model.fit(train_data, train_labels,\n","                      epochs=epochs,\n","                      validation_data=(test_data, test_labels),\n","                      batch_size=minibatches, verbose=verbose)\n","  end = timer()\n","\n","  training_time = end - start  # time in seconds\n","\n","  # Compute the privacy budget expended\n","  epsilon = -42\n","  if dpsgd:\n","    epsilon = compute_epsilon(noise_multiplier, minibatches, epochs)\n","\n","  return history, training_time, epsilon"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qcX9xItv_i_K","colab_type":"text"},"source":["## Train a Model"]},{"cell_type":"code","metadata":{"id":"mBWP7o5atjWL","colab_type":"code","outputId":"043c4048-74df-4055-fc0d-8a1bdcf616df","executionInfo":{"status":"ok","timestamp":1586450366656,"user_tz":-120,"elapsed":4170617,"user":{"displayName":"jonas sander","photoUrl":"","userId":"00193758946461779324"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Train the keras model and compute the used privacy budget.\n","dpsgd = True\n","_, training_time, epsilon = main(dpsgd, 0.15, 1.1, 1.0, 250, 60, 250)\n","\n","print('Training time: ', training_time)\n","if dpsgd:\n","  print('For delta=1e-5, the current epsilon is: %.2f' % epsilon)\n","else:\n","  print('Trained with vanilla non-private SGD optimizer')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 60000 samples, validate on 10000 samples\n","Epoch 1/60\n","60000/60000 [==============================] - 71s 1ms/sample - loss: 1.5211 - acc: 0.5188 - val_loss: 0.8233 - val_acc: 0.7082\n","Epoch 2/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.6373 - acc: 0.7911 - val_loss: 0.4991 - val_acc: 0.8458\n","Epoch 3/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.4814 - acc: 0.8594 - val_loss: 0.4325 - val_acc: 0.8804\n","Epoch 4/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.4389 - acc: 0.8857 - val_loss: 0.3999 - val_acc: 0.8990\n","Epoch 5/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.4127 - acc: 0.8992 - val_loss: 0.3707 - val_acc: 0.9106\n","Epoch 6/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.3901 - acc: 0.9093 - val_loss: 0.3645 - val_acc: 0.9167\n","Epoch 7/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.3761 - acc: 0.9151 - val_loss: 0.3454 - val_acc: 0.9235\n","Epoch 8/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.3658 - acc: 0.9196 - val_loss: 0.3408 - val_acc: 0.9263\n","Epoch 9/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.3566 - acc: 0.9236 - val_loss: 0.3247 - val_acc: 0.9310\n","Epoch 10/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.3464 - acc: 0.9265 - val_loss: 0.3204 - val_acc: 0.9333\n","Epoch 11/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.3445 - acc: 0.9289 - val_loss: 0.3121 - val_acc: 0.9376\n","Epoch 12/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.3378 - acc: 0.9309 - val_loss: 0.3139 - val_acc: 0.9376\n","Epoch 13/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.3271 - acc: 0.9336 - val_loss: 0.2993 - val_acc: 0.9404\n","Epoch 14/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.3197 - acc: 0.9365 - val_loss: 0.2966 - val_acc: 0.9433\n","Epoch 15/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.3082 - acc: 0.9387 - val_loss: 0.2832 - val_acc: 0.9442\n","Epoch 16/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2980 - acc: 0.9401 - val_loss: 0.2662 - val_acc: 0.9480\n","Epoch 17/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2924 - acc: 0.9419 - val_loss: 0.2676 - val_acc: 0.9472\n","Epoch 18/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2854 - acc: 0.9435 - val_loss: 0.2606 - val_acc: 0.9482\n","Epoch 19/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2811 - acc: 0.9446 - val_loss: 0.2636 - val_acc: 0.9478\n","Epoch 20/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2787 - acc: 0.9454 - val_loss: 0.2544 - val_acc: 0.9492\n","Epoch 21/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2747 - acc: 0.9466 - val_loss: 0.2438 - val_acc: 0.9511\n","Epoch 22/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2690 - acc: 0.9478 - val_loss: 0.2403 - val_acc: 0.9536\n","Epoch 23/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2642 - acc: 0.9494 - val_loss: 0.2353 - val_acc: 0.9534\n","Epoch 24/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2552 - acc: 0.9501 - val_loss: 0.2307 - val_acc: 0.9537\n","Epoch 25/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2524 - acc: 0.9512 - val_loss: 0.2244 - val_acc: 0.9550\n","Epoch 26/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2494 - acc: 0.9519 - val_loss: 0.2310 - val_acc: 0.9538\n","Epoch 27/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2513 - acc: 0.9521 - val_loss: 0.2262 - val_acc: 0.9567\n","Epoch 28/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2469 - acc: 0.9525 - val_loss: 0.2290 - val_acc: 0.9560\n","Epoch 29/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2452 - acc: 0.9530 - val_loss: 0.2207 - val_acc: 0.9554\n","Epoch 30/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2425 - acc: 0.9536 - val_loss: 0.2184 - val_acc: 0.9585\n","Epoch 31/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2385 - acc: 0.9546 - val_loss: 0.2170 - val_acc: 0.9586\n","Epoch 32/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2350 - acc: 0.9552 - val_loss: 0.2141 - val_acc: 0.9577\n","Epoch 33/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2326 - acc: 0.9560 - val_loss: 0.2093 - val_acc: 0.9593\n","Epoch 34/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2378 - acc: 0.9556 - val_loss: 0.2158 - val_acc: 0.9588\n","Epoch 35/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2364 - acc: 0.9565 - val_loss: 0.2106 - val_acc: 0.9580\n","Epoch 36/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2353 - acc: 0.9572 - val_loss: 0.2111 - val_acc: 0.9589\n","Epoch 37/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2370 - acc: 0.9573 - val_loss: 0.2075 - val_acc: 0.9602\n","Epoch 38/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2364 - acc: 0.9573 - val_loss: 0.2071 - val_acc: 0.9597\n","Epoch 39/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2339 - acc: 0.9579 - val_loss: 0.2138 - val_acc: 0.9598\n","Epoch 40/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2354 - acc: 0.9581 - val_loss: 0.2129 - val_acc: 0.9600\n","Epoch 41/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2401 - acc: 0.9579 - val_loss: 0.2069 - val_acc: 0.9602\n","Epoch 42/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2403 - acc: 0.9579 - val_loss: 0.2070 - val_acc: 0.9614\n","Epoch 43/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2400 - acc: 0.9577 - val_loss: 0.2096 - val_acc: 0.9606\n","Epoch 44/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2374 - acc: 0.9581 - val_loss: 0.2075 - val_acc: 0.9621\n","Epoch 45/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2391 - acc: 0.9582 - val_loss: 0.2055 - val_acc: 0.9629\n","Epoch 46/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2333 - acc: 0.9591 - val_loss: 0.2015 - val_acc: 0.9616\n","Epoch 47/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2310 - acc: 0.9590 - val_loss: 0.2012 - val_acc: 0.9625\n","Epoch 48/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2328 - acc: 0.9589 - val_loss: 0.2076 - val_acc: 0.9617\n","Epoch 49/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2333 - acc: 0.9593 - val_loss: 0.2057 - val_acc: 0.9613\n","Epoch 50/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2344 - acc: 0.9593 - val_loss: 0.2007 - val_acc: 0.9632\n","Epoch 51/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2315 - acc: 0.9602 - val_loss: 0.2096 - val_acc: 0.9631\n","Epoch 52/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2308 - acc: 0.9606 - val_loss: 0.2055 - val_acc: 0.9614\n","Epoch 53/60\n","60000/60000 [==============================] - 70s 1ms/sample - loss: 0.2315 - acc: 0.9608 - val_loss: 0.2114 - val_acc: 0.9619\n","Epoch 54/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2296 - acc: 0.9605 - val_loss: 0.2082 - val_acc: 0.9621\n","Epoch 55/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2271 - acc: 0.9609 - val_loss: 0.2045 - val_acc: 0.9629\n","Epoch 56/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2261 - acc: 0.9612 - val_loss: 0.2040 - val_acc: 0.9636\n","Epoch 57/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2262 - acc: 0.9612 - val_loss: 0.2052 - val_acc: 0.9643\n","Epoch 58/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2274 - acc: 0.9609 - val_loss: 0.2039 - val_acc: 0.9640\n","Epoch 59/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2261 - acc: 0.9608 - val_loss: 0.2027 - val_acc: 0.9641\n","Epoch 60/60\n","60000/60000 [==============================] - 69s 1ms/sample - loss: 0.2219 - acc: 0.9614 - val_loss: 0.1963 - val_acc: 0.9668\n","Training time:  4168.984221939001\n","For delta=1e-5, the current epsilon is: 2.97\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YPdFsuIN_nH_","colab_type":"text"},"source":["## Evaluate DP-SGD"]},{"cell_type":"code","metadata":{"id":"jb6oPVWE9B0Y","colab_type":"code","outputId":"a477fcc7-1aad-4be7-8bd8-20578db34669","executionInfo":{"status":"ok","timestamp":1586442692828,"user_tz":-120,"elapsed":17784216,"user":{"displayName":"jonas sander","photoUrl":"","userId":"00193758946461779324"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["# We evaluate the DP-SGD with the following different\n","# hyperparameters in terms of epsilon, accuracy and running time. We run each\n","# each hyperparameter configuration (run) in a loop of 3 iterations.\n","\n","dps = [False, True, True, True]\n","learning_rates = [0.1, 0.25, 0.15, 0.25]\n","noise_multipliers = [1.3, 1.1, 0.7]\n","clipping_thresholds = [1.5, 1, 1.5]\n","minibatches = 250\n","microbatches = 250\n","epochs = [20, 15, 60, 45]\n","\n","# a running index\n","dp_idx = -1\n","# save the hyperparameters of each iteration in a list\n","rows = []\n","for idx, dp in enumerate(dps):\n","  if dp:\n","    dp_idx += 1\n","\n","  for i in range(3):\n","    print('Run: ', idx, ' - Loop: ', i)\n","\n","    history, training_time, epsilon = main(dp, learning_rates[idx],\n","                                           noise_multipliers[dp_idx],\n","                                           clipping_thresholds[dp_idx],\n","                                           minibatches, epochs[idx],\n","                                           microbatches, 0)\n","  \n","    accuracy = history.history['val_acc'][-1]\n","  \n","    # save the hyperparameters per row in a dict\n","    row = {'dp': dp,\n","           'learning_rate': learning_rates[idx],\n","           'noise_multiplier': noise_multipliers[dp_idx],\n","           'clipping_threshols': clipping_thresholds[dp_idx],\n","           'minibatches': minibatches,\n","           'microbatches': microbatches,\n","           'epochs': epochs[idx],\n","           'epsilon': epsilon,\n","           'accuracy': accuracy,\n","           'training_time': training_time}\n","\n","    rows.append(row)\n","\n","df = pd.DataFrame(rows)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Run:  0  - Loop:  0\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Run:  0  - Loop:  1\n","Run:  0  - Loop:  2\n","Run:  1  - Loop:  0\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Run:  1  - Loop:  1\n","Run:  1  - Loop:  2\n","Run:  2  - Loop:  0\n","Run:  2  - Loop:  1\n","Run:  2  - Loop:  2\n","Run:  3  - Loop:  0\n","Run:  3  - Loop:  1\n","Run:  3  - Loop:  2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K6WT4kmPfA7U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":452},"outputId":"f322ad05-7639-450d-b3ae-6fc1230b34bf","executionInfo":{"status":"ok","timestamp":1586599112825,"user_tz":-120,"elapsed":673,"user":{"displayName":"jonas sander","photoUrl":"","userId":"00193758946461779324"}}},"source":["# save/load and clean up df\n","#df.to_pickle('/content/drive/My Drive/df.pkl')\n","df = pd.read_pickle('/content/drive/My Drive/df.pkl')\n","df.loc[df.dp==False, ['epsilon', 'noise_multiplier', 'clipping_threshols']] = np.nan\n","df['Run'] = np.array([[i]*3 for i in range(4)]).flatten()\n","df = df.set_index('Run')\n","df.loc[:, 'training_time'] /= 60\n","df"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dp</th>\n","      <th>learning_rate</th>\n","      <th>noise_multiplier</th>\n","      <th>clipping_threshols</th>\n","      <th>minibatches</th>\n","      <th>microbatches</th>\n","      <th>epochs</th>\n","      <th>epsilon</th>\n","      <th>accuracy</th>\n","      <th>training_time</th>\n","    </tr>\n","    <tr>\n","      <th>Run</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>False</td>\n","      <td>0.10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>20</td>\n","      <td>NaN</td>\n","      <td>0.9896</td>\n","      <td>0.537926</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>False</td>\n","      <td>0.10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>20</td>\n","      <td>NaN</td>\n","      <td>0.9896</td>\n","      <td>0.313726</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>False</td>\n","      <td>0.10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>20</td>\n","      <td>NaN</td>\n","      <td>0.9908</td>\n","      <td>0.312303</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>True</td>\n","      <td>0.25</td>\n","      <td>1.3</td>\n","      <td>1.5</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>15</td>\n","      <td>1.179901</td>\n","      <td>0.9506</td>\n","      <td>17.198169</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>True</td>\n","      <td>0.25</td>\n","      <td>1.3</td>\n","      <td>1.5</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>15</td>\n","      <td>1.179901</td>\n","      <td>0.9473</td>\n","      <td>17.125886</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>True</td>\n","      <td>0.25</td>\n","      <td>1.3</td>\n","      <td>1.5</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>15</td>\n","      <td>1.179901</td>\n","      <td>0.9509</td>\n","      <td>17.115841</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>True</td>\n","      <td>0.15</td>\n","      <td>1.1</td>\n","      <td>1.0</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>60</td>\n","      <td>2.969930</td>\n","      <td>0.9679</td>\n","      <td>68.369128</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>True</td>\n","      <td>0.15</td>\n","      <td>1.1</td>\n","      <td>1.0</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>60</td>\n","      <td>2.969930</td>\n","      <td>0.9639</td>\n","      <td>68.144310</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>True</td>\n","      <td>0.15</td>\n","      <td>1.1</td>\n","      <td>1.0</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>60</td>\n","      <td>2.969930</td>\n","      <td>0.9656</td>\n","      <td>68.915586</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>True</td>\n","      <td>0.25</td>\n","      <td>0.7</td>\n","      <td>1.5</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>45</td>\n","      <td>7.009134</td>\n","      <td>0.9695</td>\n","      <td>52.453361</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>True</td>\n","      <td>0.25</td>\n","      <td>0.7</td>\n","      <td>1.5</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>45</td>\n","      <td>7.009134</td>\n","      <td>0.9706</td>\n","      <td>54.237840</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>True</td>\n","      <td>0.25</td>\n","      <td>0.7</td>\n","      <td>1.5</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>45</td>\n","      <td>7.009134</td>\n","      <td>0.9685</td>\n","      <td>53.504667</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        dp  learning_rate  noise_multiplier  ...   epsilon  accuracy  training_time\n","Run                                          ...                                   \n","0    False           0.10               NaN  ...       NaN    0.9896       0.537926\n","0    False           0.10               NaN  ...       NaN    0.9896       0.313726\n","0    False           0.10               NaN  ...       NaN    0.9908       0.312303\n","1     True           0.25               1.3  ...  1.179901    0.9506      17.198169\n","1     True           0.25               1.3  ...  1.179901    0.9473      17.125886\n","1     True           0.25               1.3  ...  1.179901    0.9509      17.115841\n","2     True           0.15               1.1  ...  2.969930    0.9679      68.369128\n","2     True           0.15               1.1  ...  2.969930    0.9639      68.144310\n","2     True           0.15               1.1  ...  2.969930    0.9656      68.915586\n","3     True           0.25               0.7  ...  7.009134    0.9695      52.453361\n","3     True           0.25               0.7  ...  7.009134    0.9706      54.237840\n","3     True           0.25               0.7  ...  7.009134    0.9685      53.504667\n","\n","[12 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"qLjZXRmBgJPL","colab_type":"code","outputId":"f92f4771-2ebb-4d5a-f841-74ed61de26c9","executionInfo":{"status":"ok","timestamp":1586599116309,"user_tz":-120,"elapsed":950,"user":{"displayName":"jonas sander","photoUrl":"","userId":"00193758946461779324"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# calculate the relative standard derivation\n","(df.loc[:,['accuracy', 'training_time']].std(level='Run')/\n","df.loc[:,['accuracy', 'training_time']].mean(level='Run'))"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>training_time</th>\n","    </tr>\n","    <tr>\n","      <th>Run</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000700</td>\n","      <td>0.334690</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.002104</td>\n","      <td>0.002619</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.002079</td>\n","      <td>0.005793</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.001083</td>\n","      <td>0.016797</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     accuracy  training_time\n","Run                         \n","0    0.000700       0.334690\n","1    0.002104       0.002619\n","2    0.002079       0.005793\n","3    0.001083       0.016797"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"XpSrv_yagFh5","colab_type":"code","outputId":"501bd50a-b45d-4c95-bb1a-04d6a09f90ef","executionInfo":{"status":"ok","timestamp":1586599118369,"user_tz":-120,"elapsed":439,"user":{"displayName":"jonas sander","photoUrl":"","userId":"00193758946461779324"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# calculate the mean of each run\n","df = df.mean(level='Run')\n","df"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dp</th>\n","      <th>learning_rate</th>\n","      <th>noise_multiplier</th>\n","      <th>clipping_threshols</th>\n","      <th>minibatches</th>\n","      <th>microbatches</th>\n","      <th>epochs</th>\n","      <th>epsilon</th>\n","      <th>accuracy</th>\n","      <th>training_time</th>\n","    </tr>\n","    <tr>\n","      <th>Run</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>False</td>\n","      <td>0.10</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>20</td>\n","      <td>NaN</td>\n","      <td>0.990000</td>\n","      <td>0.387985</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>True</td>\n","      <td>0.25</td>\n","      <td>1.3</td>\n","      <td>1.5</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>15</td>\n","      <td>1.179901</td>\n","      <td>0.949600</td>\n","      <td>17.146632</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>True</td>\n","      <td>0.15</td>\n","      <td>1.1</td>\n","      <td>1.0</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>60</td>\n","      <td>2.969930</td>\n","      <td>0.965800</td>\n","      <td>68.476342</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>True</td>\n","      <td>0.25</td>\n","      <td>0.7</td>\n","      <td>1.5</td>\n","      <td>250</td>\n","      <td>250</td>\n","      <td>45</td>\n","      <td>7.009134</td>\n","      <td>0.969533</td>\n","      <td>53.398623</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        dp  learning_rate  noise_multiplier  ...   epsilon  accuracy  training_time\n","Run                                          ...                                   \n","0    False           0.10               NaN  ...       NaN  0.990000       0.387985\n","1     True           0.25               1.3  ...  1.179901  0.949600      17.146632\n","2     True           0.15               1.1  ...  2.969930  0.965800      68.476342\n","3     True           0.25               0.7  ...  7.009134  0.969533      53.398623\n","\n","[4 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"ZKEhigvl_7lD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"83a47b9d-0681-4467-e374-347843e4c62b","executionInfo":{"status":"ok","timestamp":1586599120664,"user_tz":-120,"elapsed":441,"user":{"displayName":"jonas sander","photoUrl":"","userId":"00193758946461779324"}}},"source":["df = df.drop(labels=['minibatches', 'microbatches'], axis=1)\n","print(df.to_latex(index=False))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["\\begin{tabular}{lrrrrrrr}\n","\\toprule\n","    dp &  learning\\_rate &  noise\\_multiplier &  clipping\\_threshols &  epochs &   epsilon &  accuracy &  training\\_time \\\\\n"," False &           0.10 &               NaN &                 NaN &      20 &       NaN &  0.990000 &       0.387985 \\\\\n","\\midrule\n","  True &           0.25 &               1.3 &                 1.5 &      15 &  1.179901 &  0.949600 &      17.146632 \\\\\n","  True &           0.15 &               1.1 &                 1.0 &      60 &  2.969930 &  0.965800 &      68.476342 \\\\\n","  True &           0.25 &               0.7 &                 1.5 &      45 &  7.009134 &  0.969533 &      53.398623 \\\\\n","\\bottomrule\n","\\end{tabular}\n","\n"],"name":"stdout"}]}]}