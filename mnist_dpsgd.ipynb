{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VV_ZlSFz_Nfb"
   },
   "source": [
    "# MNIST DP-SGD Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Jvqj7hz_b2P"
   },
   "source": [
    "## Imports (and Google Drive Mount in Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24089,
     "status": "ok",
     "timestamp": 1586599063472,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -120
    },
    "id": "ecJsF0D9CmkH",
    "outputId": "39a74b54-88cc-4fae-bc85-41f8ee8bec2e"
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluate the DP-SGD optimizer using TF 1.x.\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# set tensorlfow version in google colab\n",
    "try:\n",
    "  %tensorflow_version 1.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "# used to measure the privacy gurantee\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import \\\n",
    "    get_privacy_spent\n",
    "\n",
    "# optimizer used for the privacy-preserving training\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import \\\n",
    "    DPGradientDescentGaussianOptimizer\n",
    "\n",
    "GradientDescentOptimizer = tf.train.GradientDescentOptimizer\n",
    "\n",
    "# mount google drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Privacy Budget for given Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_epsilon(noise_multiplier, batch_size, target_delta,\n",
    "                    trainingsset_size,\n",
    "                    orders=[1 + x / 10. for x in range(1, 100)]\n",
    "                    + list(range(12, 64))):\n",
    "  \"\"\"Computes epsilon values for given hyperparameters.\n",
    "\n",
    "      Epsilon describes the strength of our privacy guarantee. In the case of\n",
    "      DP-ML, it gives a bound on how much the probability of a particular model\n",
    "      output can vary by including (or removing) a single training example. We\n",
    "      usually want it to be a small constant. However, this is only an upper\n",
    "      bound, and a large value of epsilon could still mean good practical\n",
    "      privacy. Interpreting this value could be quiet difficult.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  float\n",
    "      Epsion-value for the expanded privacy budget.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  noise_multiplier : numpy.ndarray\n",
    "      Parameter to control how much noise is sampled and added to gradients\n",
    "      before they are applied by the optimizer.\n",
    "  batch_size : int\n",
    "      Number of samples used in each training step.\n",
    "  target_delta : float\n",
    "      Delta bounds the probability of our privacy guarantee not holding. A rule\n",
    "      of thumb is to set it to be less than the inverse of the training data\n",
    "      size (i.e., the population size).\n",
    "  trainingsset_size : int\n",
    "      Number of samples in the trainingset.\n",
    "  orders : list, optional\n",
    "      List of orders, at which the Renyi divergence will be computed. If you\n",
    "      are targeting a particular range of epsilons (say, 1â€”10) and your delta\n",
    "      is fixed (say, 10^-5), then your orders must cover the range between\n",
    "      1+ln(1/delta)/10=2.15 and 1+ln(1/delta)/1=12.5. The default orders are\n",
    "      suitable for the mnist dataset.\n",
    "  \"\"\"\n",
    "  # Together with the noise multiplier are these the parameters which are\n",
    "  # relevant to measuring the potential privacy loss induced by the training.\n",
    "  # being included in a minibatch.\n",
    "  #\n",
    "  # Number of steps the optimizer takes in each epoch.\n",
    "  steps_per_epoch = trainingsset_size // batch_size\n",
    "  #\n",
    "  # The probability of an individual training point to be sampled in a\n",
    "  # minibatch.\n",
    "  sampling_probability = batch_size / trainingsset_size\n",
    "\n",
    "  # List of epsilons per epoch.\n",
    "  epsilon_progression = []\n",
    "\n",
    "  rdp = 0.0\n",
    "\n",
    "  for nm in noise_multiplier:\n",
    "      if nm == 0.0:\n",
    "          rdp = float('inf')\n",
    "      rdp = rdp + compute_rdp(q=sampling_probability,\n",
    "                              noise_multiplier=nm,\n",
    "                              steps=steps_per_epoch,\n",
    "                              orders=orders)\n",
    "      epsilon = get_privacy_spent(orders, rdp, target_delta=target_delta)[0]\n",
    "      epsilon_progression.append(epsilon)\n",
    "\n",
    "  return epsilon_progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24089,
     "status": "ok",
     "timestamp": 1586599063472,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -120
    },
    "id": "ecJsF0D9CmkH",
    "outputId": "39a74b54-88cc-4fae-bc85-41f8ee8bec2e"
   },
   "outputs": [],
   "source": [
    "def load_mnist(dataset='mnist'):\n",
    "  \"\"\"Loads and preprocesses the MNIST dataset.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  tuple\n",
    "      (training data, training labels, test data, test labels)\n",
    "  \"\"\"\n",
    "  \n",
    "  train, test = tf.keras.datasets.mnist.load_data()\n",
    "  train_data, train_labels = train\n",
    "  test_data, test_labels = test\n",
    "\n",
    "  train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "  test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "  train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
    "  test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
    "\n",
    "  train_labels = np.array(train_labels, dtype=np.int32)\n",
    "  test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "  train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "  test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "  return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  \"\"\"Creates a simple example CNN.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  tensorflow.python.keras.engine.sequential.Sequential\n",
    "      A simple example CNN\n",
    "  \"\"\"\n",
    "  from tensorflow.keras import Sequential\n",
    "  from tensorflow.keras.layers import Conv2D\n",
    "  from tensorflow.keras.layers import MaxPool2D\n",
    "  from tensorflow.keras.layers import Flatten\n",
    "  from tensorflow.keras.layers import Dense\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(16, 8, strides=2, padding='same', activation='relu',\n",
    "                   input_shape=(28, 28, 1)))\n",
    "  model.add(MaxPool2D(2, 1))\n",
    "  model.add(Conv2D(32, 4, strides=2, padding='valid', activation='relu'))\n",
    "  model.add(MaxPool2D(2, 1))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dense(10))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train Model (SGD or DP-SGD alias Noisy-SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24089,
     "status": "ok",
     "timestamp": 1586599063472,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -120
    },
    "id": "ecJsF0D9CmkH",
    "outputId": "39a74b54-88cc-4fae-bc85-41f8ee8bec2e"
   },
   "outputs": [],
   "source": [
    "def train_model(train_data, train_labels, test_data, test_labels, dpsgd,\n",
    "                learning_rate, noise_multiplier, l2_norm_clip, batch_size,\n",
    "                epochs, microbatches, callbacks=None, verbose=1):\n",
    "  \"\"\"Define, train and compute the used privacy budget of the keras model.\n",
    "\n",
    "  Raises\n",
    "  ------\n",
    "  ValueError\n",
    "      The number of microbatches must divide the batch size.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  train_data : numpy.ndarray\n",
    "      Array of training datapoints.\n",
    "  train_labels : numpy.ndarry\n",
    "      Array of labels for the training datapoints (one-hot encodding).\n",
    "  test_data : numpy.ndarray\n",
    "      Array of test datapoints.\n",
    "  test_labels : numpy.ndarray\n",
    "      Array of labels for the test datapoints (one-hot encodding).\n",
    "  dpsgd : bool\n",
    "      If True, train with DP-SGD. If False, train with vanilla SGD.\n",
    "  learning_rate : float\n",
    "      Learning rate for training.\n",
    "  noise_multiplier : float\n",
    "      Ratio of the standard deviation to the clipping norm. Typically more\n",
    "      noise results in stronger privacy and often at the expense of utility.\n",
    "  l2_norm_clip : float\n",
    "      Attribute gives the maximum Euclidean norm of each individual gradient\n",
    "      that is computed on an individual training example from a minibatch. This\n",
    "      parameter is used to bound the optimizer's sensitivity to individual\n",
    "      training points.\n",
    "  batch_size : int\n",
    "      Number of samples used in each training step.\n",
    "  epochs : int\n",
    "      Number of epochs used for the training.\n",
    "  microbatches : int\n",
    "      Number of microbatches (must be evently divide batch size). In practice\n",
    "      clipping gradients for each exampe indivdudally can strongly degrade the\n",
    "      performance because instead of parallelizing at the granularity of\n",
    "      batch_size the computations must be performed for each example. Rather\n",
    "      than clipping gradients per example we clip them on the basis of\n",
    "      microbatches. In this way is the number of microbatches a trade-off\n",
    "      parameter between privacy and utility (small number -> higher privacy,\n",
    "      number closer to size of batch_size -> higher utility).\n",
    "  callbacks : list, optional\n",
    "      Callbacks allow to customize the behaviour of a model during training,\n",
    "      evaluation and inference. Be careful, most callbacks do not work with the\n",
    "      DP optimizers. Alternatively you can develop your own callbacks.\n",
    "  verbose : int, optional\n",
    "      Verbose parameter of the TF fit function.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  tuple\n",
    "      Training history, training time, epsilon value (see compute_epsilon())\n",
    "  \"\"\"\n",
    "  if dpsgd and batch_size % microbatches != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly size of'\n",
    "                     + 'batch_size')\n",
    "\n",
    "  if dpsgd and (len(train_data) % batch_size != 0\n",
    "                or len(test_data) % batch_size != 0):\n",
    "    raise ValueError('Size of minibatches should divide evenly size of'\n",
    "                     + 'training- and testdatasets.')\n",
    "\n",
    "  model = create_model()\n",
    "\n",
    "  if dpsgd:\n",
    "    optimizer = DPGradientDescentGaussianOptimizer(\n",
    "        l2_norm_clip=l2_norm_clip,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        num_microbatches=microbatches,\n",
    "        learning_rate=learning_rate)\n",
    "    # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "    # The optimizers needs the loss per example in order to compute the\n",
    "    # gradients per example (rather than per minibatch) and clip/noise the\n",
    "    # gradient of each example individually.\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "  else:\n",
    "    optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(train_data, train_labels,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(test_data, test_labels),\n",
    "                      batch_size=batch_size,\n",
    "                      verbose=verbose,\n",
    "                      callbacks=callbacks)\n",
    "\n",
    "  return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, x, y):\n",
    "  \"\"\"model.evaluate() does not work with DP-Optimizer in this simple setting.\n",
    "  Use this function instead.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  model : tensorflow.python.keras.engine.sequential.Sequential\n",
    "      The model to be evaluated.\n",
    "  x : np.ndarray\n",
    "      Datapoints for the evaluation.\n",
    "  y : np.ndarray\n",
    "      Labels to the datapoints in x.\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  float\n",
    "      Accuracy of the model on the given datapoints and labels.\n",
    "  \"\"\"\n",
    "  correct_preds = np.sum(np.argmax(model.predict(x), axis=1)\n",
    "                                    == np.argmax(y, axis=1))\n",
    "  return correct_preds / len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qcX9xItv_i_K"
   },
   "source": [
    "## Fit a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks and Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Callbacks\n",
    "\n",
    "# - TF Callbacks\n",
    "# * ModelCheckpoint does not work with TF-Privacy\n",
    "# * TensorBoard does not work as simple callback with TF-Privacy, to get\n",
    "#   TensorBoard running in TF1.x use profile_batch=0 ()\n",
    "\n",
    "# - Custom Callbacks\n",
    "import time\n",
    "\n",
    "\n",
    "class RunTimeHistory(tf.keras.callbacks.Callback):\n",
    "  \"\"\"Callback to make runtime measuremnts.\n",
    "\n",
    "  Attributes\n",
    "  ----------\n",
    "  epoch_time_start : float\n",
    "      Time at the beginning of the current epoch.\n",
    "  times : list\n",
    "      Training time for each epoch.\n",
    "  \"\"\"\n",
    "\n",
    "  def on_train_begin(self, logs={}):\n",
    "      self.times = []\n",
    "\n",
    "  def on_epoch_begin(self, batch, logs={}):\n",
    "      self.epoch_time_start = time.time()\n",
    "\n",
    "  def on_epoch_end(self, batch, logs={}):\n",
    "      self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "\n",
    "class EarlyStopping(tf.keras.callbacks.Callback):\n",
    "  \"\"\"Callback to stop training when the validation accuracy is at its max, i.e.\n",
    "  the validation accuracy stops increasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, patience=0):\n",
    "    self.patience = patience\n",
    "\n",
    "    # best_weights to store the weights at which the maximum val_acc occurs.\n",
    "    self.best_weights = None\n",
    "\n",
    "  def on_train_begin(self, logs=None):\n",
    "    # The number of epoch it has waited when val_acc is not maximum\n",
    "    self.wait = 0\n",
    "    # The epoch the training stops at.\n",
    "    self.stopped_epoch = 0\n",
    "    # Initialize the best as infinity.\n",
    "    self.best = 0\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    current = logs.get('val_acc')\n",
    "    if np.less(self.best, current):\n",
    "      self.best = current\n",
    "      self.wait = 0\n",
    "      # Record the best weights if current results is better (less).\n",
    "      self.best_weights = self.model.get_weights()\n",
    "    else:\n",
    "      self.wait += 1\n",
    "      if self.wait >= self.patience:\n",
    "        self.stopped_epoch = epoch\n",
    "        self.model.stop_training = True\n",
    "\n",
    "  def on_train_end(self, logs=None):\n",
    "    if self.stopped_epoch > 0:\n",
    "      print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
    "      print('Restoring model weights from the end of the best epoch.')\n",
    "      self.model.set_weights(self.best_weights)\n",
    "\n",
    "\n",
    "# TODO: Write custom callback for model checkpointing\n",
    "# TODO: Write custom callback that stopps training after x minutes\n",
    "# TODO: Write custom callback for (dynamic) learning rate decay\n",
    "# TODO: Write custom callback to adjust Hyperparameter during training\n",
    "\n",
    "# - Selected Callbacks\n",
    "runtime_history = RunTimeHistory()\n",
    "early_stopping = EarlyStopping(5)\n",
    "callbacks = [runtime_history]\n",
    "\n",
    "# --- Hyperparamater\n",
    "dpsgd = True\n",
    "learning_rate = 0.15\n",
    "noise_multiplier = 1.1\n",
    "l2_norm_clip = 1.0\n",
    "# For the DP Optimizer the size of the minibatches must divide the number of\n",
    "# training samples!\n",
    "minibatches = 100\n",
    "epochs = 2\n",
    "# For the DP Optimizer the size of the microbatches must divide the size of the\n",
    "# minibatches!\n",
    "microbatches = 100\n",
    "verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4170617,
     "status": "ok",
     "timestamp": 1586450366656,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -120
    },
    "id": "mBWP7o5atjWL",
    "outputId": "043c4048-74df-4055-fc0d-8a1bdcf616df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.3020 - acc: 0.1110 - val_loss: 2.2799 - val_acc: 0.1730\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.2606 - acc: 0.2270 - val_loss: 2.2409 - val_acc: 0.2640\n"
     ]
    }
   ],
   "source": [
    "# Load training and test data.\n",
    "train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "# Train the keras model and compute the used privacy budget.\n",
    "model, history = train_model(train_data[:1000], train_labels[:1000], test_data[:1000], test_labels[:1000], dpsgd,\n",
    "                             learning_rate, noise_multiplier, l2_norm_clip,\n",
    "                             minibatches, epochs, microbatches, callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Evaluation (only if dpsgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_multipliers = epochs * [noise_multiplier]\n",
    "# Rule of thump: Delta is set to 1e-5 because MNIST has 60000 training\n",
    "# points (see the walktrough). Delta bounds the probability that our privacy\n",
    "# guarantee do not hold.\n",
    "target_delta = 1e-5\n",
    "# Compute the privacy budget expended\n",
    "epsilon_progression = compute_epsilon(noise_multipliers, minibatches,\n",
    "                                      target_delta, len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect everything potentially interesting in a dataframe\n",
    "(In truth, there is a lot more interesting data produced during the training... Maybe you can find some more interesting insights yourself...)\n",
    "\n",
    "Let's save for each epoch:\n",
    "* loss\n",
    "* validation loss\n",
    "* accuracy\n",
    "* validation accuracy\n",
    "* learning rate\n",
    "* noise multiplier\n",
    "* l2 norm clip\n",
    "* epsilon value\n",
    "* runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>noise_multiplier</th>\n",
       "      <th>l2_norm_clip</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.301958</td>\n",
       "      <td>2.279926</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.843951</td>\n",
       "      <td>5.077537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.260638</td>\n",
       "      <td>2.240914</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.865550</td>\n",
       "      <td>2.525313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  val_loss  accuracy  val_accuracy  learning_rate  \\\n",
       "0  2.301958  2.279926     0.111         0.173           0.15   \n",
       "1  2.260638  2.240914     0.227         0.264           0.15   \n",
       "\n",
       "   noise_multiplier  l2_norm_clip   epsilon   runtime  \n",
       "0               1.1           1.0  0.843951  5.077537  \n",
       "1               1.1           1.0  0.865550  2.525313  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# If we use the DP-Optimizer the history object contains the loss value for\n",
    "# each minibatch in an epoch. To obtain the loss for the entire epoch, we\n",
    "# calculate the average of these values.\n",
    "loss = [np.mean(l) for l in history.history['loss']]\n",
    "val_loss = [np.mean(l) for l in history.history['val_loss']]\n",
    "\n",
    "learning_rates = epochs * [learning_rate]\n",
    "noise_multipliers = epochs * [noise_multiplier] if dpsgd else epochs * [np.nan]\n",
    "l2_norm_clips = epochs * [l2_norm_clip] if dpsgd else epochs * [np.nan]\n",
    "epsilon_progression = epsilon_progression if dpsgd else epochs * [np.nan]\n",
    "\n",
    "data = {'loss': loss,\n",
    "        'val_loss': val_loss,\n",
    "        'accuracy': history.history['acc'],\n",
    "        'val_accuracy': history.history['val_acc'],\n",
    "        'learning_rate': learning_rates,\n",
    "        'noise_multiplier': noise_multipliers,\n",
    "        'l2_norm_clip': l2_norm_clips,\n",
    "        'epsilon': epsilon_progression,\n",
    "        'runtime': runtime_history.times}\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sammel ich alle Metriken die mich interessieren?\n",
    "# * Runtime Measurements\n",
    "# * history: loss, acc, val_loss, val_acc\n",
    "#\n",
    "# Implementiere Privacy Analyse\n",
    "#\n",
    "# Kommentiere alles bisherige ordentlich\n",
    "#\n",
    "# Neue Ãœberschirften\n",
    "#\n",
    "# Sammel alle Daten in einem Panda Frame\n",
    "#\n",
    "# Implementiere Gird-Search\n",
    "#\n",
    "# Implementiere Visualisierung\n",
    "# minibatches => batch_size\n",
    "# Update early stopping callback to use loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate DP-SGD or Hyperparameter-Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpsgds = [False] + 17 * [True]\n",
    "learning_rates = 18 * [0.1]\n",
    "noise_multipliers = [x / 10.0 for x in range(5, 16, 1)] + 6 * [1.0]\n",
    "l2_norm_clips = 11 * [1.0] + [x / 10.0 for x in range(10, 16, 1)]\n",
    "minibatches = 18 * [250]\n",
    "microbatches = 18 * [250]\n",
    "epochs = 18 * [20]\n",
    "target_delta = 1e-5\n",
    "runs = 3\n",
    "verbose = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Experiment: 1 / 18 Run:  1 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 1 / 18 Run:  2 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 1 / 18 Run:  3 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 2 / 18 Run:  1 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 2 / 18 Run:  2 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 2 / 18 Run:  3 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 3 / 18 Run:  1 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 3 / 18 Run:  2 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 3 / 18 Run:  3 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 4 / 18 Run:  1 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 4 / 18 Run:  2 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 4 / 18 Run:  3 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 5 / 18 Run:  1 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 5 / 18 Run:  2 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 5 / 18 Run:  3 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 6 / 18 Run:  1 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 6 / 18 Run:  2 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 6 / 18 Run:  3 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 7 / 18 Run:  1 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 7 / 18 Run:  2 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 7 / 18 Run:  3 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 8 / 18 Run:  1 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 8 / 18 Run:  2 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 8 / 18 Run:  3 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 9 / 18 Run:  1 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 9 / 18 Run:  2 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 9 / 18 Run:  3 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "\n",
      "Experiment: 10 / 18 Run:  1 / 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "Epoch 2/20\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "dp_idx = -1\n",
    "for idx, dp in enumerate(dpsgds):\n",
    "  dp_idx = dp_idx + 1 if dp else dp_idx\n",
    "  for run in range(runs):\n",
    "    print('\\n')\n",
    "    print('Experiment:', idx+1, '/', len(dpsgds), 'Run: ', run+1, '/', runs)\n",
    "    model, history = train_model(train_data, train_labels, test_data, test_labels, dp,\n",
    "                                 learning_rates[idx], noise_multipliers[dp_idx], l2_norm_clips[dp_idx],\n",
    "                                 minibatches[idx], epochs[idx], microbatches[idx], callbacks, verbose)\n",
    "    \n",
    "    loss = [np.mean(l) for l in history.history['loss']]\n",
    "    val_loss = [np.mean(l) for l in history.history['val_loss']]\n",
    "    \n",
    "    learning_rates_per_epoch = epochs[idx] * [learning_rates[idx]]\n",
    "    \n",
    "    noise_multipliers_per_epoch = []\n",
    "    l2_norm_clips_per_epoch = []\n",
    "    \n",
    "    if dp:\n",
    "      noise_multipliers_per_epoch = epochs[idx] * [noise_multipliers[dp_idx]]\n",
    "      l2_norm_clips_per_epoch = epochs[idx] * [l2_norm_clips[dp_idx]]\n",
    "      epsilon_progression = compute_epsilon(noise_multipliers_per_epoch, minibatches[idx],\n",
    "                                          target_delta, len(train_data))\n",
    "    else:\n",
    "      noise_multipliers_per_epoch = epochs[idx] * [np.nan]\n",
    "      l2_norm_clips_per_epoch = epochs[idx] * [np.nan]\n",
    "      epsilon_progression = epochs[idx] * [np.nan]\n",
    "    \n",
    "    \n",
    "    run_data = ({'experiment_nr': idx,\n",
    "                 'run_nr': run,\n",
    "                 'loss': loss,\n",
    "                 'val_loss': val_loss,\n",
    "                 'accuracy': history.history['acc'],\n",
    "                 'val_accuracy': history.history['val_acc'],\n",
    "                 'learning_rate': learning_rates_per_epoch,\n",
    "                 'noise_multiplier': noise_multipliers_per_epoch,\n",
    "                 'l2_norm_clip': l2_norm_clips_per_epoch,\n",
    "                 'epsilon': epsilon_progression,\n",
    "                 'runtime': runtime_history.times})\n",
    "    \n",
    "    run_data = pd.DataFrame(run_data)\n",
    "    \n",
    "    data.append(run_data)\n",
    "    #print(run_data.to_string())\n",
    "    \n",
    "    data_df = pd.concat(data)\n",
    "    data_df.to_pickle('data_exp' + str(idx) + '_run' + str(run) + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPdFsuIN_nH_"
   },
   "source": [
    "## Evaluate DP-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17784216,
     "status": "ok",
     "timestamp": 1586442692828,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -120
    },
    "id": "jb6oPVWE9B0Y",
    "outputId": "a477fcc7-1aad-4be7-8bd8-20578db34669"
   },
   "outputs": [],
   "source": [
    "# We evaluate the DP-SGD with the following different\n",
    "# hyperparameters in terms of epsilon, accuracy and running time. We run each\n",
    "# each hyperparameter configuration (run) in a loop of 3 iterations.\n",
    "\n",
    "dps = [False, True, True, True]\n",
    "learning_rates = [0.1, 0.25, 0.15, 0.25]\n",
    "noise_multipliers = [1.3, 1.1, 0.7]\n",
    "clipping_thresholds = [1.5, 1, 1.5]\n",
    "minibatches = 250\n",
    "microbatches = 250\n",
    "epochs = [20, 15, 60, 45]\n",
    "\n",
    "# a running index\n",
    "dp_idx = -1\n",
    "# save the hyperparameters of each iteration in a list\n",
    "rows = []\n",
    "for idx, dp in enumerate(dps):\n",
    "  if dp:\n",
    "    dp_idx += 1\n",
    "\n",
    "  for i in range(3):\n",
    "    print('Run: ', idx, ' - Loop: ', i)\n",
    "\n",
    "    history, training_time, epsilon = main(dp, learning_rates[idx],\n",
    "                                           noise_multipliers[dp_idx],\n",
    "                                           clipping_thresholds[dp_idx],\n",
    "                                           minibatches, epochs[idx],\n",
    "                                           microbatches, 0)\n",
    "  \n",
    "    accuracy = history.history['val_acc'][-1]\n",
    "  \n",
    "    # save the hyperparameters per row in a dict\n",
    "    row = {'dp': dp,\n",
    "           'learning_rate': learning_rates[idx],\n",
    "           'noise_multiplier': noise_multipliers[dp_idx],\n",
    "           'clipping_threshols': clipping_thresholds[dp_idx],\n",
    "           'minibatches': minibatches,\n",
    "           'microbatches': microbatches,\n",
    "           'epochs': epochs[idx],\n",
    "           'epsilon': epsilon,\n",
    "           'accuracy': accuracy,\n",
    "           'training_time': training_time}\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1586599112825,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -120
    },
    "id": "K6WT4kmPfA7U",
    "outputId": "f322ad05-7639-450d-b3ae-6fc1230b34bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dp</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>noise_multiplier</th>\n",
       "      <th>clipping_threshols</th>\n",
       "      <th>minibatches</th>\n",
       "      <th>microbatches</th>\n",
       "      <th>epochs</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.537926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>0.313726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.312303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>15</td>\n",
       "      <td>1.179901</td>\n",
       "      <td>0.9506</td>\n",
       "      <td>17.198169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>15</td>\n",
       "      <td>1.179901</td>\n",
       "      <td>0.9473</td>\n",
       "      <td>17.125886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>15</td>\n",
       "      <td>1.179901</td>\n",
       "      <td>0.9509</td>\n",
       "      <td>17.115841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>60</td>\n",
       "      <td>2.969930</td>\n",
       "      <td>0.9679</td>\n",
       "      <td>68.369128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>60</td>\n",
       "      <td>2.969930</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>68.144310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>60</td>\n",
       "      <td>2.969930</td>\n",
       "      <td>0.9656</td>\n",
       "      <td>68.915586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>45</td>\n",
       "      <td>7.009134</td>\n",
       "      <td>0.9695</td>\n",
       "      <td>52.453361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>45</td>\n",
       "      <td>7.009134</td>\n",
       "      <td>0.9706</td>\n",
       "      <td>54.237840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>45</td>\n",
       "      <td>7.009134</td>\n",
       "      <td>0.9685</td>\n",
       "      <td>53.504667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dp  learning_rate  noise_multiplier  ...   epsilon  accuracy  training_time\n",
       "Run                                          ...                                   \n",
       "0    False           0.10               NaN  ...       NaN    0.9896       0.537926\n",
       "0    False           0.10               NaN  ...       NaN    0.9896       0.313726\n",
       "0    False           0.10               NaN  ...       NaN    0.9908       0.312303\n",
       "1     True           0.25               1.3  ...  1.179901    0.9506      17.198169\n",
       "1     True           0.25               1.3  ...  1.179901    0.9473      17.125886\n",
       "1     True           0.25               1.3  ...  1.179901    0.9509      17.115841\n",
       "2     True           0.15               1.1  ...  2.969930    0.9679      68.369128\n",
       "2     True           0.15               1.1  ...  2.969930    0.9639      68.144310\n",
       "2     True           0.15               1.1  ...  2.969930    0.9656      68.915586\n",
       "3     True           0.25               0.7  ...  7.009134    0.9695      52.453361\n",
       "3     True           0.25               0.7  ...  7.009134    0.9706      54.237840\n",
       "3     True           0.25               0.7  ...  7.009134    0.9685      53.504667\n",
       "\n",
       "[12 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save/load and clean up df\n",
    "#df.to_pickle('/content/drive/My Drive/df.pkl')\n",
    "df = pd.read_pickle('/content/drive/My Drive/df.pkl')\n",
    "df.loc[df.dp==False, ['epsilon', 'noise_multiplier', 'clipping_threshols']] = np.nan\n",
    "df['Run'] = np.array([[i]*3 for i in range(4)]).flatten()\n",
    "df = df.set_index('Run')\n",
    "df.loc[:, 'training_time'] /= 60\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 950,
     "status": "ok",
     "timestamp": 1586599116309,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -120
    },
    "id": "qLjZXRmBgJPL",
    "outputId": "f92f4771-2ebb-4d5a-f841-74ed61de26c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.334690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.002619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.005793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.016797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     accuracy  training_time\n",
       "Run                         \n",
       "0    0.000700       0.334690\n",
       "1    0.002104       0.002619\n",
       "2    0.002079       0.005793\n",
       "3    0.001083       0.016797"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the relative standard derivation\n",
    "(df.loc[:,['accuracy', 'training_time']].std(level='Run')/\n",
    "df.loc[:,['accuracy', 'training_time']].mean(level='Run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1586599118369,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -120
    },
    "id": "XpSrv_yagFh5",
    "outputId": "501bd50a-b45d-4c95-bb1a-04d6a09f90ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dp</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>noise_multiplier</th>\n",
       "      <th>clipping_threshols</th>\n",
       "      <th>minibatches</th>\n",
       "      <th>microbatches</th>\n",
       "      <th>epochs</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.387985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>15</td>\n",
       "      <td>1.179901</td>\n",
       "      <td>0.949600</td>\n",
       "      <td>17.146632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>60</td>\n",
       "      <td>2.969930</td>\n",
       "      <td>0.965800</td>\n",
       "      <td>68.476342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>45</td>\n",
       "      <td>7.009134</td>\n",
       "      <td>0.969533</td>\n",
       "      <td>53.398623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dp  learning_rate  noise_multiplier  ...   epsilon  accuracy  training_time\n",
       "Run                                          ...                                   \n",
       "0    False           0.10               NaN  ...       NaN  0.990000       0.387985\n",
       "1     True           0.25               1.3  ...  1.179901  0.949600      17.146632\n",
       "2     True           0.15               1.1  ...  2.969930  0.965800      68.476342\n",
       "3     True           0.25               0.7  ...  7.009134  0.969533      53.398623\n",
       "\n",
       "[4 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the mean of each run\n",
    "df = df.mean(level='Run')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1586599120664,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -120
    },
    "id": "ZKEhigvl_7lD",
    "outputId": "83a47b9d-0681-4467-e374-347843e4c62b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      "    dp &  learning\\_rate &  noise\\_multiplier &  clipping\\_threshols &  epochs &   epsilon &  accuracy &  training\\_time \\\\\n",
      " False &           0.10 &               NaN &                 NaN &      20 &       NaN &  0.990000 &       0.387985 \\\\\n",
      "\\midrule\n",
      "  True &           0.25 &               1.3 &                 1.5 &      15 &  1.179901 &  0.949600 &      17.146632 \\\\\n",
      "  True &           0.15 &               1.1 &                 1.0 &      60 &  2.969930 &  0.965800 &      68.476342 \\\\\n",
      "  True &           0.25 &               0.7 &                 1.5 &      45 &  7.009134 &  0.969533 &      53.398623 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(labels=['minibatches', 'microbatches'], axis=1)\n",
    "print(df.to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPJ6sH58rXL9yEO2V4YPuBF",
   "collapsed_sections": [],
   "name": "mnist_dpsgd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
